import random
import numpy as np
from functools import cache

def pattern_encoding(pattern):
  pattern_encode = [[0] * 27 for _ in range(45)]
  for i in range(len(pattern)):
    if pattern[i] != '_':
      pattern_encode[i][ord(pattern[i]) - 97] = 1
    else:
      pattern_encode[i][26] = 1
  return pattern_encode

def pattern_decoding(pattern_encode):
  pattern = ''
  for i in range(45):
    try:
      index = pattern_encode[i].index(1)
    except ValueError:
      break
    if index < 26:
      pattern += chr(index + 97)
    else:
      pattern += '_'
  return pattern

def word_encoding(word):
  word_encode = [0] * 27
  for letter in set(list(word)):
    word_encode[ord(letter) - 97] = 1
  return word_encode

def word_decoding(word_encode):
  word = ''
  for i in range(26):
    if word_encode[i] == 1:
      word += chr(i + 97)
  return word

def create_history(pattern, answer):
  letters_inpattern = pattern.replace('_', '')
  letters_inanswer = set(list(answer))
  letters_potential = ''.join(set(list('abcdefghijklmnopqrstuvwxyz')).difference(letters_inanswer))
  histories = []
  k = random.choice(range(1, 6))
  letters_tried = ''.join(random.sample(letters_potential, k=k))
  histories.append(word_encoding(letters_tried + letters_inpattern))
  return histories

def create_dataset_word(word):
  @cache
  def update(cur):
    letters_left = ''.join(set(list(cur))).replace('_', '')
    letters_removed = ''.join(set(list(word)).difference(set(list(cur))))
    patterns = set()
    if cur.count('_') > 0:
      patterns.add((cur, letters_removed))
    for letter in letters_left:
      cur_next = cur.replace(letter, '_')
      patterns |= update(cur_next)
    return patterns

  pattern_set = update(word)
  dataset_word = []
  for pattern, letters_removed in pattern_set:
    word_encode = word_encoding(letters_removed)
    for history in create_history(pattern, word):
      X = pattern_encoding(pattern)
      X.append(history)
      dataset_word.append([X, word_encode])
  return dataset_word




words_to_be_used = dataset_words.sample(21000).values.flatten()
words_train = words_to_be_used[:20000]
words_val = words_to_be_used[20000:]

buffer = []
shard_idx = 0
shard_size = 1000000   # for example, adjust as needed

for word in tqdm(words_train):
  word_records = create_dataset_word(word)
  for word_record in word_records:
    X_list, y_list = word_record  # X_list has shape (46,27), y_list has shape (27,)

    # If they come back as Python lists, convert each sub‐list to a torch.Tensor:
    # (Assuming you want one sample per (X_row, y_val)):
    # But if you instead mean “X_list” is an entire matrix and “y_list” is the label vector
    # for that entire matrix, then treat (X_list, y_list) as a single sample:
    X_tensor = torch.tensor(X_list, dtype=torch.float32)  # shape (46, 27)
    y_tensor = torch.tensor(y_list, dtype=torch.float32)  # shape (27,)

    buffer.append((X_tensor, y_tensor))

    if len(buffer) >= shard_size:
        torch.save(buffer, f"data/shard_{shard_idx:04d}.pt")
        # print('\n', f"→ Saved {len(buffer)} samples to data/shard_{shard_idx:04d}.pt")
        buffer.clear()
        shard_idx += 1

# Don’t forget the tail‐end:
if buffer:
    torch.save(buffer, f"data/shard_{shard_idx:04d}.pt")
    # print('\n', f"→ Saved {len(buffer)} samples to data/shard_{shard_idx:04d}.pt")
    buffer.clear()








import glob
import torch
from torch.utils.data import IterableDataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

class ShardIterableDataset(IterableDataset):
    def __init__(self, pattern):
        super().__init__()
        self.pattern = pattern

    def __iter__(self):
        for path in sorted(glob.glob(self.pattern)):
            shard = torch.load(path)   # list of (X, y)
            for (X, y) in shard:
                yield (X, y)

    def __len__(self):
        total = 0
        for path in sorted(glob.glob(self.pattern)):
            shard = torch.load(path)
            total += len(shard)
        return total


def collate_fn(batch):
    Xs, ys = zip(*batch)
    lengths = torch.tensor([x.shape[0] for x in Xs], dtype=torch.long)
    X_pad = pad_sequence(Xs, batch_first=True, padding_value=0.0)

    if all(y.shape[0] == ys[0].shape[0] for y in ys):
        y_pad = torch.stack(ys)
    else:
        y_pad = pad_sequence(ys, batch_first=True, padding_value=-100)
    return X_pad, y_pad, lengths


# Instantiate dataset + DataLoader
ds = ShardIterableDataset("data/shard_*.pt")
loader = DataLoader(
    ds,
    batch_size=32,
    shuffle=False,    # IterableDataset ⇒ shuffle must be handled internally if needed
    collate_fn=collate_fn,
)








import torch
import torch.nn as nn

class Seq2VecBiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0
        )
        # We’ll take the final hidden states (forward + backward) → 2*hidden_size
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x, lengths=None):
        """
        x: [batch_size, T_max, input_size]
        lengths: [batch_size] (actual lengths, for packing). Can be None if all same length.
        """
        if lengths is not None:
            packed = nn.utils.rnn.pack_padded_sequence(
                x, lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            _, (h_n, _) = self.lstm(packed)
            # h_n: [num_layers * 2, batch_size, hidden_size]
        else:
            _, (h_n, _) = self.lstm(x)
            # h_n: [num_layers * 2, batch_size, hidden_size]

        # h_n[-2] is the last layer’s forward hidden state
        # h_n[-1] is the last layer’s backward hidden state
        # Stack them along dim=1 → shape [batch_size, hidden_size*2]
        forward_last  = h_n[-2]  # [batch_size, hidden_size]
        backward_last = h_n[-1]  # [batch_size, hidden_size]
        h_combined = torch.cat((forward_last, backward_last), dim=1)  # → [batch_size, hidden_size*2]

        logits = self.fc(h_combined)  # → [batch_size, output_size]
        return logits







# Suppose each X has shape [46, 27], each y has shape [27].
# So input_size = 27, output_size = 27.
model = Seq2VecBiLSTM(input_size=27, hidden_size=128, output_size=27)
model = model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()


for epoch in tqdm(range(3)):
    epoch_loss = 0.0
    batch_count = 0

    for X_batch, y_batch, lengths in loader:
        # Move inputs to GPU
        X_batch = X_batch.to(device, non_blocking=True)    # [B, T_max, 27]
        y_batch = y_batch.to(device, non_blocking=True)    # [B, 27] (one-hot or similar)
        lengths = lengths.to(device, non_blocking=True)    # [B]

        # Compute logits
        preds = model(X_batch, lengths)  # [B, 27]

        # Convert each 27-dim y vector into a class index in [0..26]
        # (assumes y_batch is something like one-hot or has a single “1.0” at the correct index)
        target_indices = torch.argmax(y_batch, dim=1)  # [B], dtype=torch.int64

        # Compute cross-entropy loss
        loss = criterion(preds, target_indices)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        batch_count += 1

    avg_loss = epoch_loss / batch_count
    print(f"Epoch {epoch}, avg loss = {avg_loss:.5f}")






# Make sure model is in eval mode once
model.eval()

total_loss = 0.0
batch_count = 0
Results = []
val_length = len(val_ds)

with torch.no_grad():
    for X_batch, y_batch, lengths in tqdm(val_loader):
        # Move entire batch to GPU once
        X_batch = X_batch.to(device)     # [B, T_max, 27]
        y_batch = y_batch.to(device)     # [B, 27]
        lengths = lengths.to(device)     # [B]

        # Single forward pass for the whole batch
        preds = model(X_batch, lengths)  # [B, 27]
        target_indices = torch.argmax(y_batch, dim=1)
        loss = criterion(preds, target_indices)
        total_loss += loss.item()
        batch_count += 1

        # Bring predictions and inputs back to CPU for decoding
        preds_np = preds.cpu().numpy()          # [B, 27]
        X_np = X_batch.cpu().numpy()            # [B, T, 27]
        y_np = y_batch.cpu().numpy()            # [B, 27]

        B = preds_np.shape[0]
        for i in range(B):
            pred_vec = preds_np[i]               # [27]
            X_list = X_np[i].tolist()           # list of 46 lists of length 27
            y_list = y_np[i].tolist()           # list of length 27

            word_left = word_decoding(y_list)
            history = word_decoding(X_list[45])
            # Find the top-ranked letter (chr(i+97)) among indices where X_list[45][i] == 0
            # without rerunning the model:
            for idx in pred_vec.argsort()[::-1]:
                if X_list[45][idx] == 0:
                    guess = chr(idx + 97)
                    break

            guess_successful = (guess in word_left)
            Results.append(guess_successful)
        print(np.mean(Results))

# Compute average MSE over all validation batches
avg_mse = total_loss / batch_count
print(f"Validation MSE over {val_length} samples: {avg_mse:.5f}")

# Compute overall accuracy
accuracy = np.mean(Results)
print(f"Validation accuracy over {val_length} samples: {accuracy:.5f}")





